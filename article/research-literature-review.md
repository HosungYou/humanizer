# Research Literature Review on AI Text Detection and Humanization (2024-2026)

> This literature review systematically organizes key research from 2024-2026 in the fields of AI-generated text detection and detection evasion/humanization. It comprehensively covers detection methodologies, stylometric features, paraphrasing evasion research, non-native speaker bias, commercial detector comparisons, academic-specific detection, and composite human-likeness scoring systems.

---

## 1. AI Text Detection Methodologies

AI-generated text detection technologies are broadly classified into three approaches: perplexity-based, classifier-based, and watermarking-based. Each approach has its own strengths, weaknesses, and scope of application, and the latest commercial tools employ these in combination.

### 1.1 Perplexity-Based Detection

Perplexity-based methods evaluate the "predictability" of text by leveraging token-level probability from language models. Low perplexity indicates that the model can predict the given token sequence with high probability, which is a typical characteristic of AI-generated text.

**Key Research and Tools:**

- **DetectGPT (Mitchell et al.)**: The most widely cited perplexity-based detection method in academia. It statistically compares the difference in log-probability between the original text and perturbed text to determine whether the text was AI-generated. It is based on the observation that text generated by a model tends to exhibit a significant decrease in log-probability after perturbation.

- **GPTZero**: Adopts a hybrid approach combining perplexity and burstiness. Rather than relying on a single signal, it uses a dual-signal system that simultaneously measures token-level predictability and sentence-level variability.

**Key Weakness -- Model-Specificity:**

Perplexity is inherently model-dependent. The same text can appear "surprising" (high perplexity) to one model while appearing "predictable" (low perplexity) to another. This poses a serious limitation in the current environment where diverse LLMs coexist. In particular, post-GPT-4 generation models can produce text with higher perplexity than earlier generations, making detection based solely on the perplexity metric increasingly difficult.

### 1.2 Classifier-Based Detection

Classifier-based methods use fine-tuned transformer classifiers to learn distributional features beyond perplexity. Classifiers trained on AI/human text pairs capture complex patterns at the lexical, syntactic, and discourse levels.

**Key Tools and Approaches:**

- **RoBERTa-based fine-tuned classifiers**: RoBERTa models trained on AI/human text pairs are widely used as the foundational architecture.
- **ZeroGPT**: A commercial implementation of the classifier-based approach.
- **Originality.ai**: A classifier-based detector specializing in long-form academic text, recording top performance in independent benchmarks.
- **Copyleaks**: A classifier-based tool with strengths in multilingual detection.
- **Turnitin AI Detection**: The most widely deployed classifier-based system across academic institutions.

**2024 Meta-Analysis Results:**

Stylometric methods alone achieve 70-80% precision on GPT-4-level text, but when fused with ML classifiers, this rises to approximately 90%. This suggests the importance of leveraging composite features rather than relying on a single signal.

### 1.3 Watermarking-Based Detection

Watermarking is a proactive approach that inserts statistical signals by biasing token selection toward a pseudo-random "green list" during LLM text generation. It represents a fundamentally different paradigm in that the signal is embedded during generation rather than after.

**Key Research Progress:**

- **Kirchenbauer et al. (University of Maryland, 2023)**: Established the foundational watermarking algorithm with the "green list" algorithm. At each token generation step, the vocabulary is partitioned into "green" and "red" groups using a pseudo-random function based on the preceding token, and the logits of green list tokens are increased to create a statistically detectable bias in the generated text.

- **Scott Aaronson's Gumbel-max scheme**: A cryptographic variant of the Kirchenbauer method that provides stronger security guarantees.

- **Christ & Gunn (2024 CRYPTO)**: Introduced "pseudorandom error-correcting codes" as a theoretical foundation, presenting a theoretical framework for mathematically guaranteed robust watermarks. This study is regarded as having significantly strengthened the theoretical foundations of the watermarking field.

- **Google DeepMind SynthID-Text (Nature, 2024)**: The first large-scale commercially deployed watermarking system. Published in Nature, this study demonstrated the feasibility of watermarking in real production environments.

**Key Limitations:**

Watermarking only works when the generating model implements it. Text generated by models that have not implemented watermarking cannot be detected retroactively. This represents a fundamental limitation requiring cooperation from all LLM providers.

### 1.4 Features Used by Detectors

The features utilized by current major detection tools are summarized in the table below.

| Feature | Description | Used By |
|---|---|---|
| **Perplexity** | Token-level predictability score. Lower values indicate higher AI likelihood | GPTZero, DetectGPT |
| **Burstiness** | Variance in sentence length and complexity. Lower values indicate higher AI likelihood | GPTZero, Sapling |
| **Stylometric features** | Complex linguistic features including POS frequency, punctuation entropy, lexical diversity | Classifier-based detectors, academic detectors |
| **Entropy/randomness** | Distribution patterns of token selection. Used for watermark signal detection | Watermarking systems |
| **Semantic embeddings** | Capturing semantic patterns through high-dimensional text representations | Turnitin, Copyleaks |
| **Discourse markers** | Usage patterns of discourse markers such as "however," "this," "because" | Academic corpus studies |

---

## 2. Stylometric Features

Stylometric features that distinguish AI-generated text from human-written text manifest across multiple levels: lexical, syntactic, and discourse. This section systematically organizes the features at each level.

### 2.1 Lexical Patterns

The most intuitive and widely observed characteristic of AI-generated text is the excessive use of certain "style words."

**AI Overuse Vocabulary List:**

Key AI signature words include "delve," "underscore," "showcase," "encapsulate," "noteworthy," "seamless," "crucial," "findings," "potential," "navigate the landscape," among others. These arise from high-frequency words in LLM training data being preferentially selected during generation.

**2024 PubMed Large-Scale Study:**

A 2024 analysis of 14 million PubMed abstracts (2010-2024) identified **454 words** that are significantly overused in AI-assisted writing. The usage frequency of these words shows a sharp inflection point in 2023-2024. This constitutes quantitative evidence suggesting that AI-assisted writing rapidly proliferated in the academic domain following the release of ChatGPT.

**Key Insights:**

- **Style word overuse is the signature of AI.** The overuse of functional/rhetorical style words, not content words, is the true indicator of AI-assisted writing.
- AI text exhibits **significantly higher consistency in word choice across documents** than human text. Human authors display diverse vocabulary choices even on the same topic, whereas AI tends to repeatedly select the same words in similar contexts.

### 2.2 Syntactic Patterns

Beyond the lexical level, AI text shows systematic differences in syntactic structure.

**Key Syntactic Features:**

- **Lower use of equivocal discourse markers**: The frequency of concessive/contrastive markers such as "however," "but," "although" is lower compared to human text. This reflects AI's limitations in expressing the complexity and multifaceted nature of argumentation.

- **Underuse of "this" and "because"**: These two words function as markers of human referential reasoning and causal reasoning. Their underuse in AI text indicates that AI performs contextual referencing and causal explanation less frequently.

- **Uniformity of sentence structure**: AI text exhibits more uniform sentence structure, with lower syntactic variety throughout the document. While human writers unpredictably mix subordination and coordination, AI tends to maintain consistent syntactic patterns.

### 2.3 Hedging and Boosting

Hedging is a linguistic strategy that softens the certainty of claims, while boosting strengthens it. AI text shows systematically different patterns from human text in both dimensions.

**Key Findings:**

- AI text **systematically moderates intensity**. For example, where a human would assert strongly with "undoubtedly confirms," AI uses the softened expression "clearly affirms."

- Human academic text exhibits **strong hedging/boosting contrast**. That is, strong boosting is used where confidence is high, and deep hedging is used where uncertainty exists, forming a distinct gradient. In contrast, AI **flattens** this gradient, maintaining a generally moderate level of confidence throughout.

- A 2024 corpus study published in SCIRP directly compared hedging and engagement markers between AI essays and human essays, confirming measurable differences in both frequency and distribution.

### 2.4 Burstiness -- Sentence Length Variation

Burstiness is a metric that measures the variance in sentence length and structural complexity within a text, and it is one of the most critical quantitative signals in AI detection alongside perplexity.

**Definition and Calculation Methods:**

The basic procedure for measuring burstiness:
1. Tokenize the text into sentence units
2. Calculate each sentence's length by word count
3. Compute variance/variation metrics of sentence lengths

**4 Calculation Methods:**

| Method | Formula | Characteristics |
|---|---|---|
| **Method 1: Standard Deviation** | `burstiness_score = std_deviation(sentence_lengths)` | Simplest and most intuitive. Dependent on document length |
| **Method 2: Fano Factor** | `Fano Factor = Variance(lengths) / Mean(lengths)` | Fano Factor > 1: super-Poissonian (bursty, human-like); = 1: Poisson; < 1: sub-Poissonian (regular, AI-like) |
| **Method 3: Coefficient of Variation (CV)** | `CV = Std_deviation(lengths) / Mean(lengths)` | Normalized metric enabling cross-document comparison |
| **Method 4: Perplexity Variance** | Calculate each sentence's perplexity using a reference LM, then compute the variance of per-sentence perplexity scores within the document | The most information-rich metric integrating perplexity and burstiness |

**Human vs AI Patterns:**

- Human writing **naturally mixes short sentences (3-5 words) with long complex constructions (30-50 words)**, producing high burstiness. This reflects the natural rhythm of thought flow.

- LLMs **tend to produce medium-length sentences with consistent syntactic complexity**, exhibiting systematically low burstiness. This stems from structural characteristics of the token-by-token generation mechanism.

- Even AI-generated text with high perplexity exhibits low burstiness. This means that burstiness is a **complementary, partially independent signal** from perplexity.

- GPTZero's detection system explicitly combines perplexity and burstiness as its **two primary signals**.

### 2.5 Vocabulary Diversity Metrics

Lexical diversity is a family of metrics that measure the richness and variation of vocabulary used within a text. Various measurement methods have been proposed, each with its own strengths and limitations.

**Key Metric Comparison:**

| Metric | Description | Strengths | Limitations |
|---|---|---|---|
| **TTR** (Type-Token Ratio) | Unique word count / Total word count | Simple and intuitive | Heavily dependent on text length |
| **MATTR** (Moving Average TTR) | Rolling window TTR average | Reduced length bias | Window size is a free parameter |
| **MTLD** (Measure of Textual Lexical Diversity) | Mean length of consecutive word strings maintaining TTR above a threshold | **Length-invariant**; validated across 4 dimensions of validity | High computational cost |
| **vocd-D / HD-D** | Curve-fitting to hypergeometric distribution | Statistically rigorous | Complex implementation |
| **Maas** | Log-based transformation of TTR | Low length sensitivity | Lacks intuitiveness |

**McCarthy & Jarvis (2010) Validity Assessment:**

McCarthy & Jarvis (2010) validated MTLD, vocd-D, and HD-D as superior metrics to raw TTR. In particular, **MTLD was confirmed as the only metric that does not vary as a function of text length**, making it the most suitable metric for comparing texts of different lengths.

**AI vs Human Text Comparison Results:**

- LLMs **generally exhibit lower lexical diversity**, favoring common syntactic structures. However, this is genre-dependent.

- **Genre-specific reversal**: In some genres such as mystery, adventure, and romance, GPT-4 text shows higher lexical diversity than human-authored text. This suggests that the simple generalization "AI = low diversity" is risky.

- **Academic writing specificity**: In the domain of academic writing, ChatGPT appears superficially vocabulary-rich, yet is **inferior to human authors in the subtlety of collocational usage and contextual appropriateness**.

- Traditional metrics (TTR, MTLD, vocd-D) are increasingly being supplemented by neural methods. A 2024 study proposed an **autoencoder-based framework** that can capture contextual relationships missed by surface-level metrics.

### 2.6 Structural and Discourse-Level Patterns

Beyond lexical and syntactic levels, discourse-level structural patterns constitute the deepest-level signatures of AI text.

#### 2.6.1 Paragraph Architecture

AI-generated academic text exhibits formulaic paragraph structure: **topic sentence -> supporting evidence -> transitional synthesis**. This regularity can be detected as a structural fingerprint.

Different LLMs also possess distinguishable stylistic fingerprints at the discourse level. ChatGPT, Gemini, and LLaMA have been found to be distinguishable from each other and from human authors.

#### 2.6.2 Enumeration Fingerprint

AI text excessively uses numbered lists and parallel bullet structures within prose. This is a pattern flagged in academic writing analysis.

This "enumeration fingerprint" is **structurally persistent** and far more difficult to remove through paraphrasing than surface vocabulary. Because paraphrasers operate at the sentence/phrase level, the list structure itself is substantially preserved even after transformation.

#### 2.6.3 Transition and Discourse Markers

AI text relies excessively on explicit logical connectors at predictable intervals:
- "Furthermore," "Moreover," "In conclusion," "It is worth noting that," etc.

Human academic text employs more diverse and often implicit logical connections. A 2024 study on discourse markers during paraphrasing confirmed that these patterns **partially survive** after being processed by paraphrasing tools.

---

## 3. Humanization/Paraphrasing Evasion Research

Research on the effectiveness and limitations of paraphrasing and humanization techniques for evading AI detection is progressing actively alongside detection research.

### 3.1 Paraphrasing Effectiveness

#### DIPPER Study (Krishna et al., 2023 -- NeurIPS)

The most widely cited paper in the paraphrasing attack field. DIPPER is an 11-billion parameter (11B) paraphrase model with controllable lexical diversity and content reordering capabilities.

Key Results:
- Reduced DetectGPT detection rate from **70.3% to 4.6%** at a 1% false positive rate
- Evaded GPTZero, watermarking systems, and OpenAI's classifier
- Semantic modification relative to the original text was minimal
- Defensive finding: Retrieval-based detection (database lookup methods) is robust against paraphrasing

#### Adversarial Paraphrasing Study (2025)

A 2025 study on adversarial paraphrasing guided by detector feedback:
- Achieved an average **87.88% true positive rate reduction** across 8 detectors at a 1% false positive rate
- **Universal transferability**: Adversarial paraphrasing targeting one detector also reduces detection rates across all other detectors
- Text quality degradation was minimal

#### Practical Evasion Results

- Paraphrasing with Spinbot alone: Approximately 85% AI detection on Copyleaks (limited evasion effect)
- Combining multiple tools + manual editing: Consistently achieved detection rates **below 10%**
- **Multiple tools + manual editing confirmed as the most effective evasion strategy**

### 3.2 Single-Pass vs Multi-Pass

| Strategy | Description | Typical Results |
|---|---|---|
| **Single-pass** | One tool applied once, no manual editing | Detection rates decrease but difficult to achieve below 20% on top detectors |
| **Multi-pass** | Iterative application of different tools | Increases text entropy and more substantially reduces AI fingerprints |
| **Multi-pass + manual editing** | Multiple tool applications followed by human manual editing | Achieves lowest detection rates (below 10%); **human intervention is the key differentiator** |

The multi-pass approach is effective because each pass targets different types of AI fingerprints, and iterative application increases the overall entropy of the text. However, manual human editing must ultimately be combined to effectively address structural patterns as well.

### 3.3 Hard-to-Humanize Features

A clear distinction exists between features that persist after paraphrasing and humanization and those that are easily removed.

**Structural Features -- High Persistence (Persistent after paraphrasing):**

| Feature | Reason for Persistence |
|---|---|
| Discourse structure | Paragraph structure and topic sentence patterns are preserved because paraphrasers operate at the sentence/phrase level |
| Enumeration patterns | List structures embedded in prose are resistant to paraphrasing |
| Transition logic | The sequence of argument moves is partially preserved even when surface words change |
| Stylometric regularities | Uniform sentence length and low burstiness partially survive even after single-pass paraphrasing |

**Lexical Features -- Easily Replaced:**

| Feature | Reason for Easy Replacement |
|---|---|
| AI-signature vocabulary | Words like "delve," "underscore" are easily replaced with synonyms by paraphrasers |
| Surface-level hedging patterns | Modifiable by humanizers |

This distinction suggests that effective humanization must include **structural transformation** beyond lexical replacement.

---

## 4. Non-Native Speaker Bias

The most serious validity issue in AI text detection is systematic bias against non-native English writers. This is a problem with educational and ethical implications that extends beyond a mere technical limitation, leading multiple elite universities to discontinue AI detection.

### 4.1 Key Research Findings

| Finding | Source | Rate |
|---|---|---|
| TOEFL essays misclassified as AI | Liang et al. (2023) | **>61%** flagged by at least one detector |
| 18 out of 91 TOEFL essays unanimously flagged | Liang et al. (2023) | **19.8%** unanimous false positive |
| UC Davis 2024 manual review | Case study | **15 out of 17** flags were false positives |
| Stanford 2025 analysis of 10,000+ samples | Stanford study | **>20%** false positive for non-native speakers |
| Free detector false positives (pre-ChatGPT human academic text) | Popkov et al. (2024) | Median **27.2%** |

### 4.2 Bias Mechanism

Non-native English writing **shares surface-level features** with AI text:
- Simpler vocabulary
- More regular sentence structures
- Less idiomatic usage

Due to these shared features, detectors trained on native English human/AI contrasts misclassify non-native text as AI-generated.

### 4.3 Accuracy-Bias Trade-off

The most serious structural problem is that in benchmark tests, **the most accurate detectors exhibit the strongest bias against non-native speakers**. That is, increasing detection accuracy increases bias, while reducing bias decreases accuracy -- a trade-off exists.

The finding that simple prompting strategies (e.g., instructing GPT to "write like a human" or to enrich vocabulary) can mitigate bias while simultaneously bypassing detection suggests that current detectors may be learning "non-native characteristics" rather than "AI characteristics."

### 4.4 Institutional Response

At least **12 elite universities** have deactivated AI detection:

| University | Period |
|---|---|
| Yale University | 2024-2025 |
| Vanderbilt University | 2024-2025 |
| Johns Hopkins University | 2024-2025 |
| Northwestern University | 2024-2025 |
| UCLA | 2024-2025 |
| UC San Diego | 2024-2025 |
| Cal State LA | 2024-2025 |
| UT Austin | 2024-2025 |

These universities cited the disproportionate impact of false positives and bias issues as the primary rationale for discontinuing AI detection.

---

## 5. Commercial Detector Comparison

A detailed comparison of the major commercial AI text detectors currently on the market is presented below. Particular attention should be paid to the discrepancy between vendor claims and independent research findings.

| Tool | Claimed Accuracy | Independent Findings | False Positive Rate | Notes |
|---|---|---|---|---|
| **GPTZero** | 99.3% | High performance on unmodified text; fails on translated content | 0.24% (claimed) | Misclassified all translated AI text as human text |
| **Originality.ai** | 98-100% | Top performance across multiple benchmarks; 100% on ChatGPT/Grok/Gemini | Low | Best performance on long-form academic text |
| **Turnitin AI** | 92-100% | Misses approximately 15% of AI text; most widely deployed in academia | ~1% (claimed) | Multiple universities deactivating |
| **Copyleaks** | 90.7% (one benchmark) | Best performance in multilingual detection; approximately 1/20 false positive | ~5% | Strengths in non-English AI text |
| **ZeroGPT** | Variable | Generally low accuracy; high false positives reported | High | Free tier; low reliability |
| **Sapling** | 97% | Competitive performance on clean text | -- | Used in enterprise contexts |

**Key Observations:**

- Vendor claims (GPTZero: 0.24%; Turnitin: ~1%) and independent research results diverge dramatically. Popkov et al. (2024) reported a **median 27.2% false positive rate** when free AI detectors analyzed 2016-2018 pre-ChatGPT human academic text.

- A 2024 IACIS paper revealed that when the false positive rate is constrained to below 0.5%, most detectors achieve near-zero true positive rates. This means that **detectors are effectively useless at strict thresholds**.

- Vulnerability to paraphrasing: Across all tested tools, detection accuracy "plummets" after paraphrasing, with sensitivity ranging from 0% to 100% depending on the tool and manipulation type.

---

## 6. Academic-Specific Detection Research

Academic texts, particularly scientific papers, exhibit detection characteristics distinct from general text. The formal register of academic text can mimic AI's uniformity, presenting both unique challenges and opportunities.

### 6.1 Scientific Abstract Detection

- **Human annotator performance**: Human annotators perform at near-chance levels when distinguishing AI abstracts from human abstracts. This indicates that AI very effectively mimics the formal conventions of academic abstracts.

- **Generational differences across models**: Text from earlier LLM versions is easier to detect. Post-GPT-4 models are significantly harder to detect.

- **PMC study**: Confirmed that AI-containing abstracts in PubMed in 2023 increased by **approximately 2x** compared to 2021-2022.

- **Domain-specific training**: According to University of Chicago biological sciences research, ML tools specifically trained on a given domain can detect machine-written scientific abstracts with **>99% accuracy**. However, this only applies when specifically trained on that domain.

- Turnitin and GPTZero perform differently on scientific text versus general text. The formal register of academic text can mimic AI's uniformity, potentially increasing false positives.

### 6.2 Section-Specific Detection Patterns

Detection characteristics vary depending on the section of an academic paper.

| Section | Detection Characteristics | Reason |
|---|---|---|
| **Abstract** | Highest AI detection rates | Even in human writing, abstracts are structurally formulaic, producing both high true positives and high false positives |
| **Methods** | May under-flag | Even in human writing, methods sections are highly formulaic, so actual AI usage may go undetected |
| **Discussion/Conclusion** | Most AI-detectable | These sections demand more individual voice and hedging variability, where AI's flattened hedging diverges significantly from human patterns |

These section-specific differences suggest that AI detection and humanization strategies should be differentiated according to the structural position within the paper.

---

## 7. Composite Human-Likeness Scoring

The components of a composite human-likeness scoring system for distinguishing between AI-generated and human-written text are summarized in the table below.

| Metric | Human-like Direction | Computable From |
|---|---|---|
| **Burstiness** (sentence length CV) | Higher is more human-like | Raw text |
| **MTLD** | Higher is more human-like (more diverse vocabulary) | Raw text |
| **POS entropy** | Higher is more human-like (more diverse POS mixture) | NLP tagging |
| **Per-sentence perplexity variance** | Higher is more human-like | LM inference |
| **Discourse marker diversity** | Higher is more human-like | Pattern matching |
| **AI-signature word frequency** | Lower is more human-like ("delve," "underscore," etc.) | Word frequency |
| **Fano Factor** | >1 is human-like (super-Poissonian) | Raw text |

**Absence of a Standardized Framework:**

No standardized scoring system exists as a published standard to date. The most rigorous academic approach involves **combining MTLD + per-sentence perplexity variance + Fano Factor into a multi-dimensional feature vector, then training a classifier on it**.

The strength of this approach is that it compensates for the limitations of each individual metric and can learn complex patterns that no single metric can capture alone. However, challenges remain including the need for large annotated datasets for classifier training, domain dependency, and the necessity of adapting to LLM advancements over time.

---

## 8. Evidence-Based Key Conclusions

The seven key conclusions of this literature review are summarized with their supporting evidence.

### Conclusion 1: No detector alone can be trusted for high-stakes decisions

Independent research consistently reports 10-27% false positive rates, while vendor claims range from 0.24-1%. This dramatic discrepancy raises serious questions about the reliability of vendor claims. The fact that multiple universities have withdrawn AI detection supports this.

### Conclusion 2: Paraphrasing is a major evasion method and is highly effective

DIPPER reduced DetectGPT accuracy from 70% to 4.6%. Adversarial paraphrasing achieved an 87.88% TPR reduction across 8 detectors. This reveals a fundamental vulnerability in current detection systems.

### Conclusion 3: Burstiness and perplexity are the key quantitative signals

Burstiness (CV or Fano Factor) and per-sentence perplexity variance are the most practically measurable features. MTLD is recommended as the lexical diversity metric. These function as complementary, partially independent signals from each other.

### Conclusion 4: AI vocabulary signatures are specific and documented

454 overused words were identified in 2024 PubMed data. These words show a sharp increase in usage frequency in 2023-2024, providing quantitative evidence of the spread of AI-assisted academic writing.

### Conclusion 5: Non-native speakers are systematically disadvantaged

False positive rates exceed 61% in some studies. This suggests that detectors may be capturing "non-native characteristics" rather than "AI characteristics," raising serious ethical concerns about the use of AI detection in educational assessment.

### Conclusion 6: Structural fingerprints persist even after humanization

Discourse structure, paragraph architecture, and enumeration patterns are preserved far better than vocabulary after paraphrasing. This means that effective humanization must include structural transformation beyond the lexical level.

### Conclusion 7: Watermarking is the only technically robust solution

Watermarking provides mathematically guaranteed detection, but cooperation from LLM providers is essential, and text from models that have not implemented watermarking cannot be detected retroactively. Despite this limitation, it is currently the most technically robust approach.

---

## References

1. Liang, W., Yuksekgonul, M., Mao, Y., Wu, E., & Zou, J. (2023). GPT detectors are biased against non-native English writers. *arXiv:2304.02819*.

2. Krishna, K., Song, Y., Karpinska, M., Wieting, J., & Iyyer, M. (2023). Paraphrasing evades detectors of AI-generated text. *NeurIPS*. *arXiv:2303.13408*.

3. Adversarial Paraphrasing (2025). A Universal Attack for Humanizing AI-Generated Text. *arXiv:2506.07001*.

4. Contrastive Paraphrase Attacks on LLM Detectors (2025). *arXiv:2505.15337*.

5. On the Detectability of LLM-Generated Text (2025). *arXiv:2510.20810*.

6. Feature-Based Detection: Stylometric and Perplexity Markers (2024). *ResearchGate*.

7. Detecting AI-Generated Text with Pre-Trained Models (2024). *ACL Anthology*.

8. Delving into ChatGPT usage in academic writing through excess vocabulary (2024). *arXiv:2406.07016*.

9. SynthID-Text: Scalable watermarking for LLM outputs (2024). *Nature*.

10. AI watermarking must be watertight (2024). *Nature News*.

11. Watermarking for AI-Generated Content: SoK (2024). *arXiv:2411.18479*.

12. Cryptographic watermarks (2024). *Cloudflare*.

13. Lexical diversity, syntactic complexity: ChatGPT vs L2 students (2025). *Frontiers in Education*.

14. More human than human? ChatGPT and L2 writers (2024). *De Gruyter*.

15. McCarthy, P. M., & Jarvis, S. (2010). MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. *Behavior Research Methods*, 42(2), 381-392.

16. MATTR: Pros and cons (2024). *ResearchGate*.

17. Vocabulary Quality in NLP: Autoencoder-Based Framework (2024). *Springer*.

18. Hedging Devices in AI vs. Human Essays (2024). *SCIRP*.

19. AI and human writers share stylistic fingerprints (2024). *Johns Hopkins Hub*.

20. Accuracy-bias trade-offs in AI text detection and scholarly publication fairness (2024). *PMC*.

21. Characterizing AI Content Detection in Oncology Abstracts 2021-2023 (2024). *PMC*.

22. Distinguishing academic science writing with >99% accuracy (2023). *PMC*.

23. Originality.AI -- AI Detection Studies Meta-Analysis.

24. GPTZero vs Copyleaks vs Originality comparison.

25. AI vs AI: Turnitin, ZeroGPT, GPTZero, Writer AI (2024). *ResearchGate*.

26. Generative AI models and detection: tokenization and dataset size (2024). *Frontiers in AI*.

27. Aggregated AI detector outcomes in STEM writing (2024). *American Physiological Society*.

28. IACIS (2025). Critical look at reliability of AI detection tools.

29. Stanford HAI -- AI-Detectors Biased Against Non-Native English Writers.

30. AI-generated text detection: comprehensive review (2025). *ScienceDirect*.

31. Differentiating Human-Written and AI-Generated Texts: Linguistic Features (2024). *MDPI*.

32. How AI Tools Affect Discourse Markers When Paraphrased (2024). *ResearchGate*.

33. A Comparative Analysis of AI-Generated and Human-Written Text (2024). *SSRN*.

34. Popkov, A. et al. (2024). Median 27.2% false positive rate across free AI detectors.

35. Netus AI -- How stylometric patterns survive paraphrasing.

36. Deep Dive Into AI Text Fingerprints. *Hastewire*.

---

> This literature review was written based on key research published between 2024-2026 and requires continuous updates given the rapid advancement of AI text detection technologies.
